{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- [ACTION REQUIRED] User-defined paths ---\n",
    "nc_dir = 'C:/Users/arthu/OneDrive/Desktop/data'  # Folder containing Daymet .nc files\n",
    "shapefile_path = 'C:/Users/arthu/OneDrive/Desktop/data/Shapefile/RussianSubbasinsSimple.shp'\n",
    "\n",
    "# Daymet Lambert Conformal Conic (LCC) CRS\n",
    "daymet_crs = (\n",
    "    \"+proj=lcc +lat_1=25 +lat_2=60 +lat_0=42.5 +lon_0=-100 \"\n",
    "    \"+x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\"\n",
    ")\n",
    "\n",
    "# --- Load and reproject the shapefile to Daymet's CRS ---\n",
    "basin = gpd.read_file(shapefile_path)\n",
    "basin = basin.to_crs(daymet_crs)\n",
    "\n",
    "# Regex to parse filenames like \"11549_2020_prcp.nc\" or \"11549_2021_srad.nc\"\n",
    "filename_pattern = re.compile(r'(\\d+)_(\\d{4})_(\\w+)\\.nc')\n",
    "\n",
    "# Dictionary to store DataFrames for each variable across files\n",
    "var_dataframes = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing file: 11549_2020_prcp.nc ===\n",
      "Tile: 11549, Year: 2020, Variable: prcp\n",
      "Original dims: ('time', 'y', 'x')\n",
      "Original coords: ['lambert_conformal_conic', 'x', 'y', 'time', 'lat', 'lon']\n",
      "\n",
      "=== Processing file: 11549_2021_prcp.nc ===\n",
      "Tile: 11549, Year: 2021, Variable: prcp\n",
      "Original dims: ('time', 'y', 'x')\n",
      "Original coords: ['lambert_conformal_conic', 'x', 'y', 'time', 'lat', 'lon']\n",
      "\n",
      "=== Processing file: 11549_2021_srad.nc ===\n",
      "Tile: 11549, Year: 2021, Variable: srad\n",
      "Original dims: ('time', 'y', 'x')\n",
      "Original coords: ['lambert_conformal_conic', 'x', 'y', 'time', 'lat', 'lon']\n",
      "\n",
      "=== Processing file: 11549_2021_tmax.nc ===\n",
      "Tile: 11549, Year: 2021, Variable: tmax\n",
      "Original dims: ('time', 'y', 'x')\n",
      "Original coords: ['lambert_conformal_conic', 'x', 'y', 'time', 'lat', 'lon']\n",
      "\n",
      "=== Processing file: 11549_2021_tmin.nc ===\n",
      "Tile: 11549, Year: 2021, Variable: tmin\n",
      "Original dims: ('time', 'y', 'x')\n",
      "Original coords: ['lambert_conformal_conic', 'x', 'y', 'time', 'lat', 'lon']\n",
      "\n",
      "=== Processing file: 11549_2021_vp.nc ===\n",
      "Tile: 11549, Year: 2021, Variable: vp\n",
      "Original dims: ('time', 'y', 'x')\n",
      "Original coords: ['lambert_conformal_conic', 'x', 'y', 'time', 'lat', 'lon']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Loop through all .nc files in the directory ---\n",
    "for nc_file in glob.glob(os.path.join(nc_dir, '*.nc')):\n",
    "    filename = os.path.basename(nc_file)\n",
    "    match = filename_pattern.match(filename)\n",
    "    if not match:\n",
    "        print(f\"Skipping file with unexpected name format: {filename}\")\n",
    "        continue\n",
    "\n",
    "    tile, year, var_name = match.groups()\n",
    "    print(f\"\\n=== Processing file: {filename} ===\")\n",
    "    print(f\"Tile: {tile}, Year: {year}, Variable: {var_name}\")\n",
    "\n",
    "    # 1) Test if the file can be opened\n",
    "    try:\n",
    "        ds = xr.open_dataset(nc_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening {filename}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 2) Assign the Daymet CRS to the dataset (if not already set)\n",
    "    ds = ds.rio.write_crs(daymet_crs, inplace=False)\n",
    "\n",
    "    # 3) Extract the desired variable or fall back to the first data_var\n",
    "    if var_name in ds.data_vars:\n",
    "        da = ds[var_name]\n",
    "    else:\n",
    "        # If the named variable doesn't exist, fallback\n",
    "        # or skip the file entirely if that's preferred\n",
    "        if len(ds.data_vars) == 0:\n",
    "            print(f\"No data variables found in {filename}, skipping.\")\n",
    "            ds.close()\n",
    "            continue\n",
    "        da = list(ds.data_vars.values())[0]\n",
    "        print(f\"Warning: Expected variable '{var_name}' not found in {filename}. Using '{da.name}' instead.\")\n",
    "\n",
    "    # Print dimension names for debugging\n",
    "    print(\"Original dims:\", da.dims)\n",
    "    print(\"Original coords:\", list(da.coords))\n",
    "\n",
    "    # 4) If your actual dimensions are ('time', 'y', 'x'), set them as spatial dims\n",
    "    #    If the dims differ, rename them accordingly here.\n",
    "    try:\n",
    "        da = da.rio.set_spatial_dims(x_dim='x', y_dim='y', inplace=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to set spatial dims for {filename}: {e}\")\n",
    "        ds.close()\n",
    "        continue\n",
    "\n",
    "    # 5) Clip the DataArray to the basin polygon\n",
    "    try:\n",
    "        da_clipped = da.rio.clip(basin.geometry, basin.crs, drop=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Clipping failed for {filename}: {e}\")\n",
    "        ds.close()\n",
    "        continue\n",
    "\n",
    "    # Convert the clipped DataArray to a pandas DataFrame\n",
    "    df = da_clipped.to_dataframe().reset_index()\n",
    "\n",
    "    # Ensure the 'time' column is datetime\n",
    "    if 'time' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['time']):\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    # 6) Aggregate spatial cells if multiple remain\n",
    "    expected_cols = {'time', da_clipped.name}\n",
    "    extra_cols = set(df.columns) - expected_cols\n",
    "    if extra_cols:\n",
    "        df = df.groupby('time')[da_clipped.name].mean().reset_index()\n",
    "\n",
    "    # Rename the column to the variable name\n",
    "    df = df[['time', da_clipped.name]].rename(columns={da_clipped.name: var_name})\n",
    "\n",
    "    # Store the DataFrame by variable\n",
    "    var_dataframes.setdefault(var_name, []).append(df)\n",
    "\n",
    "    ds.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final merged Daymet DataFrame shape: (730, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Concatenate & merge DataFrames for each variable ---\n",
    "for var in var_dataframes:\n",
    "    var_dataframes[var] = pd.concat(var_dataframes[var]).sort_values('time').reset_index(drop=True)\n",
    "\n",
    "merged_df = None\n",
    "for var, df in var_dataframes.items():\n",
    "    if merged_df is None:\n",
    "        merged_df = df\n",
    "    else:\n",
    "        merged_df = pd.merge(merged_df, df, on='time', how='outer')\n",
    "\n",
    "merged_df = merged_df.sort_values('time').reset_index(drop=True)\n",
    "\n",
    "# --- Save final merged DataFrame ---\n",
    "merged_df.to_csv('merged_daymet_data.csv', index=False)\n",
    "print(\"Final merged Daymet DataFrame shape:\", merged_df.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralhydrology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
